{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"Welcome to My Notebook","text":"<p>\u8fd9\u91cc\u662f GoPoux \u7684\u4e2a\u4eba\u7b14\u8bb0\u672c</p>"},{"location":"CS/","title":"Computer Science","text":"<p>Abstract</p> <p>\u5f52\u6863\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u8bfe\u7a0b\u7684\u4e00\u4e9b\u5b66\u4e60\u7b14\u8bb0</p>"},{"location":"CS/content/","title":"Content","text":""},{"location":"HPC/","title":"HPC","text":"<p>Abstract</p> <p>\u5f52\u6863 HPC \u7684\u4e00\u4e9b\u5b66\u4e60\u7b14\u8bb0</p>"},{"location":"HPC/HPC%20101/","title":"HPC 101","text":"<p>Abstract</p> <p>\u5927\u4e00\u6691\u5047\u8d85\u7b97\u77ed\u5b66\u671f labs</p> <p>\u5b9e\u9a8c\u624b\u518c</p>"},{"location":"HPC/HPC%20101/cudaC/","title":"NVIDIA CUDA C","text":"<p>Abstract</p> <p>GPU \u7f16\u7a0b\u539f\u7406\u4e0e NVIDIA CUDA C \u7f16\u7a0b\u5b9e\u9a8c</p>"},{"location":"HPC/HPC%20101/cudaC/#gpu","title":"GPU \u7f16\u7a0b\u539f\u7406","text":""},{"location":"HPC/HPC%20101/cudaC/#cpu-vs-gpu","title":"CPU vs. GPU","text":"CPU \uff1a <ul> <li>\u5c11\u91cf\u4e14\u590d\u6742\u7684\u6838\u5fc3</li> <li>\u4f4e\u5185\u5b58\u5ef6\u8fdf\u7684\u7f13\u5b58\uff08cache\uff09\u8f83\u5927</li> <li>\u5185\u5b58\u5927\u4f46\u6162</li> </ul> GPU \uff1a <ul> <li>\u5927\u91cf\u7b80\u5355\u6838\u5fc3</li> <li>\u4f4e\u5185\u5b58\u5ef6\u8fdf\u7684\u7f13\u5b58\uff08cache\uff09\u8f83\u5c0f</li> <li>\u5185\u5b58\u5c0f\u4f46\u5feb</li> </ul>"},{"location":"HPC/HPC%20101/cudaC/#cuda-vs","title":"CUDA \u7f16\u7a0b\u6a21\u578b vs. \u786c\u4ef6\u6267\u884c\u6a21\u578b","text":""},{"location":"HPC/HPC%20101/cudaC/#_1","title":"\u4f53\u7cfb\u7ed3\u6784\u7c7b\u522b\u548c\u7f16\u7a0b\u6a21\u578b","text":"<ul> <li>SISD\uff08Single instruction, single data\uff09\u5355\u6307\u4ee4\u6d41\u5355\u6570\u636e\u6d41\uff1a\u4f20\u7edf\u7684\u4e32\u884c\u8ba1\u7b97\u673a\u3002</li> <li>SIMD\uff08Single instruction, multiple data\uff09\u5355\u6307\u4ee4\u591a\u6570\u636e\u6d41\uff1aAVX\u3001SSE \u7b49\u6307\u4ee4\u96c6\u3002</li> <li>SPMD\uff08Single program, multiple data\uff09\u5355\u7a0b\u5e8f\u591a\u6570\u636e\u6d41\uff1a\u5bf9\u95ee\u9898\u8fdb\u884c\u5206\u89e3\uff0c\u518d\u8fdb\u884c\u5e76\u884c\u6c42\u89e3\u3002</li> </ul> <p>\u524d\u4e24\u4e2a\u662f\u4f53\u7cfb\u7ed3\u6784\u7c7b\u522b\uff0c\u6700\u540e\u4e00\u4e2a\u662f\u7f16\u7a0b\u6a21\u578b\u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#cuda-cc","title":"CUDA C/C++ \u7f16\u7a0b","text":"<p>\u7531 NVIDIA \u6df1\u5ea6\u5b66\u4e60\u57f9\u8bad\u4e2d\u5fc3 (DLI) \u63d0\u4f9b\u7684\u8bfe\u7a0b\uff0c\u5728\u7531 NVIDIA \u63d0\u4f9b\u7684\u4e91\u73af\u5883\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#cuda-cc_1","title":"\u4f7f\u7528 CUDA C/C++ \u52a0\u901f\u7a0b\u5e8f","text":""},{"location":"HPC/HPC%20101/cudaC/#_2","title":"\u52a0\u901f\u7cfb\u7edf","text":"<p>\u52a0\u901f\u7cfb\u7edf\u53c8\u79f0\u5f02\u6784\u7cfb\u7edf\uff0c\u7531 CPU \u548c GPU \u7ec4\u6210\u3002\u52a0\u901f\u7cfb\u7edf\u8fd0\u884c CPU \u7a0b\u5e8f\uff0c\u7531\u8fd9\u4e9b\u7a0b\u5e8f\u8c03\u5ea6\u8fd0\u884c\u4e8e GPU \u4e0a\u7684\u51fd\u6570\uff0c\u901a\u8fc7 GPU \u5b9e\u73b0\u51fd\u6570\u7684\u5e76\u884c\u8ba1\u7b97\u3002</p> <p>\u67e5\u8be2 GPU \u4fe1\u606f\u547d\u4ee4\uff1a</p> <pre><code>$ nvidia-smi\n</code></pre>"},{"location":"HPC/HPC%20101/cudaC/#gpu_1","title":"GPU \u52a0\u901f\u539f\u7406","text":"<p>\u4e00\u4e2a\u7b80\u5355\u7684 GPU \u52a0\u901f\u5e94\u7528\u7a0b\u5e8f\u6267\u884c\u8fc7\u7a0b\uff1a</p> <p></p> <ul> <li><code>(1)</code> \u6bb5\uff1a\u6570\u636e\u7531 <code>cudaMallocManaged()</code> \u51fd\u6570\u5206\u914d\uff0c\u5e76\u80fd\u7531 CPU \u8bbf\u95ee\u5904\u7406\u3002</li> <li><code>(2)</code> \u6bb5\uff1a\u6570\u636e\u53ef\u88ab\u8fc1\u79fb\u81f3\u53ef\u6267\u884c\u5e76\u884c\u5de5\u4f5c\u7684 GPU \uff0c\u5e76\u7531 GPU \u6838\u51fd\u6570\u8bbf\u95ee\uff0c\u540c\u65f6 CPU \u53ef\u7ee7\u7eed\u6267\u884c\u5de5\u4f5c\uff08\u5f02\u6b65\u6267\u884c\uff09\u3002</li> <li>\u901a\u8fc7 <code>cudaDeviceSynchronize()</code> \uff0c\u5c06 CPU \u4e0e GPU \u7684\u5de5\u4f5c\u540c\u6b65\u3002</li> <li><code>(3)</code> \u6bb5\uff1a\u7ecf CPU \u8bbf\u95ee\u7684\u6570\u636e\u8fc1\u79fb\u56de CPU\u3002</li> </ul>"},{"location":"HPC/HPC%20101/cudaC/#gpu_2","title":"GPU \u52a0\u901f\u5e94\u7528\u7a0b\u5e8f\u7f16\u5199","text":"<p>CUDA \u52a0\u901f\u7a0b\u5e8f\u6587\u4ef6\u6269\u5c55\u540d\u4e3a <code>.cu</code> \uff0c\u4e00\u4e2a\u7b80\u5355\u4f8b\u5b50\uff1a</p> <pre><code>void CPUFunction()\n{\nprintf(\"This function is defined to run on the CPU.\\n\");\n}\n\n__global__ void GPUFunction()\n{\nprintf(\"This function is defined to run on the GPU.\\n\");\n}\n\nint main()\n{\nCPUFunction();\n\nGPUFunction&lt;&lt;&lt;1, 1&gt;&gt;&gt;();\ncudaDeviceSynchronize();\n}\n</code></pre> <p>\u4e00\u4e9b\u8bed\u6cd5\uff1a</p> <p><code>__global__ void GPUFunction()</code></p> <ul> <li><code>__global__</code> \u5173\u952e\u5b57\u8868\u660e\u4ee5\u4e0b\u51fd\u6570\u5c06\u5728 GPU \u4e0a\u8fd0\u884c\u5e76\u53ef \u5168\u5c40 \u8c03\u7528\u3002</li> <li>\u901a\u5e38\uff0c\u5c06\u5728 CPU \u4e0a\u6267\u884c\u7684\u4ee3\u7801\u79f0\u4e3a \u4e3b\u673a (Host) \u4ee3\u7801\uff0c\u800c\u5c06\u5728 GPU \u4e0a\u8fd0\u884c\u7684\u4ee3\u7801\u79f0\u4e3a \u8bbe\u5907(Device) \u4ee3\u7801\u3002</li> <li>\u4f7f\u7528 <code>__global__</code> \u5173\u952e\u5b57\u5b9a\u4e49\u7684\u51fd\u6570\u9700\u8981\u8fd4\u56de <code>void</code> \u7c7b\u578b\u3002</li> </ul> <p><code>GPUFunction&lt;&lt;&lt;1, 1&gt;&gt;&gt;();</code></p> <ul> <li>\u901a\u5e38\uff0c\u5f53\u8c03\u7528\u8981\u5728 GPU \u4e0a\u8fd0\u884c\u7684\u51fd\u6570\u65f6\uff0c\u5c06\u6b64\u79cd\u51fd\u6570\u79f0\u4e3a \u5df2\u542f\u52a8 \u7684 \u6838\u51fd\u6570 \u3002</li> <li>\u542f\u52a8\u6838\u51fd\u6570\u65f6\uff0c\u5fc5\u987b\u63d0\u4f9b \u6267\u884c\u914d\u7f6e \uff0c\u5373\u5728\u5411\u6838\u51fd\u6570\u4f20\u9012\u53c2\u6570\u4e4b\u524d\u4f7f\u7528 <code>&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;</code> \u8bed\u6cd5\u5b8c\u6210\u914d\u7f6e\u3002</li> <li>\u5728\u5b8f\u89c2\u5c42\u9762\uff0c\u53ef\u901a\u8fc7\u6267\u884c\u914d\u7f6e\u4e3a\u6838\u51fd\u6570\u542f\u52a8\u6307\u5b9a \u7ebf\u7a0b\u5c42\u6b21\u7ed3\u6784 \uff0c\u4ece\u800c\u5b9a\u4e49\u7ebf\u7a0b\u7ec4\uff08\u79f0\u4e3a \u7ebf\u7a0b\u5757 \uff09\u7684\u6570\u91cf <code>number_of_blocks</code> \uff0c\u4ee5\u53ca\u8981\u5728\u6bcf\u4e2a\u7ebf\u7a0b\u5757\u4e2d\u6267\u884c\u7684 \u7ebf\u7a0b \u6570\u91cf <code>threads_per_block</code> \u3002\u5982\u6837\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4f7f\u7528\u4e86\u5305\u542b <code>1</code> \u7ebf\u7a0b\u7684 <code>1</code> \u7ebf\u7a0b\u5757\u542f\u52a8\u6838\u51fd\u6570\u3002</li> </ul> <p><code>cudaDeviceSynchronize();</code></p> <ul> <li>\u6838\u51fd\u6570\u542f\u52a8\u65b9\u5f0f\u4e3a \u5f02\u6b65 \uff1aCPU \u4ee3\u7801\u5c06\u7ee7\u7eed\u6267\u884c\uff0c\u65e0\u9700\u7b49\u5f85\u6838\u51fd\u6570\u5b8c\u6210\u542f\u52a8\u3002</li> <li>\u8c03\u7528 CUDA \u8fd0\u884c\u65f6\u63d0\u4f9b\u7684\u51fd\u6570 <code>cudaDeviceSynchronize</code> \u4f7f\u5f97\u4e3b\u673a (CPU) \u4ee3\u7801\u6682\u4f5c\u7b49\u5f85\uff0c\u76f4\u81f3\u8bbe\u5907 (GPU) \u4ee3\u7801\u6267\u884c\u5b8c\u6210\uff0c\u624d\u80fd\u5728 CPU \u4e0a\u6062\u590d\u6267\u884c\u3002</li> </ul> <p>\u7f16\u8bd1\u8fd0\u884c CUDA \u52a0\u901f\u7a0b\u5e8f</p> <p>\u4f7f\u7528 <code>nvcc</code> \u547d\u4ee4\u7f16\u8bd1\u548c\u8fd0\u884c\u7a0b\u5e8f\uff1a</p> <pre><code>$ nvcc -arch=sm_70 -o hello-gpu hello-gpu.cu -run\n</code></pre> <ul> <li><code>nvcc</code> \u4e3a\u7f16\u8bd1\u5668\u547d\u4ee4\u3002</li> <li><code>-arch</code> \u9009\u9879\u6307\u5b9a\u67b6\u6784\u7c7b\u578b\u3002</li> <li><code>-o</code> \u6307\u5b9a\u7f16\u8bd1\u7a0b\u5e8f\u7684\u8f93\u51fa\u6587\u4ef6\u3002</li> <li><code>-run</code> \u6807\u5fd7\u6267\u884c\u6210\u529f\u7f16\u8bd1\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002</li> </ul>"},{"location":"HPC/HPC%20101/cudaC/#cuda","title":"CUDA \u7ebf\u6027\u5c42\u6b21\u7ed3\u6784","text":"<p>\u542f\u52a8\u6838\u51fd\u6570\u65f6\uff0c\u6838\u51fd\u6570\u4ee3\u7801\u7531\u6bcf\u4e2a\u5df2\u914d\u7f6e\u7684\u7ebf\u7a0b\u5757\u4e2d\u7684\u6bcf\u4e2a\u7ebf\u7a0b\u6267\u884c\u3002</p> <p>CUDA \u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7ebf\u7a0b\u5c42\u6b21\u7ed3\u6784\u53d8\u91cf\uff1a</p> <ul> <li><code>gridDim.x</code> \u7f51\u683c\u4e2d\u5757\u6570\uff1b<code>blockDim.x</code> \u6bcf\u5757\u4e2d\u7ebf\u7a0b\u6570\u3002</li> <li><code>blockIdx.x</code> \u5f53\u524d\u7ebf\u7a0b\u6240\u5728\u7ebf\u7a0b\u5757\u7d22\u5f15\uff1b<code>threadIdx.x</code> \u5f53\u524d\u7ebf\u7a0b\u5728\u7ebf\u7a0b\u5757\u4e2d\u7684\u7d22\u5f15\u3002</li> </ul>"},{"location":"HPC/HPC%20101/cudaC/#_3","title":"\u52a0\u901f\u5faa\u73af","text":"<p>\u6b65\u9aa4\uff1a</p> <ul> <li>\u5fc5\u987b\u7f16\u5199\u5b8c\u6210 \u5faa\u73af\u7684\u5355\u6b21\u8fed\u4ee3 \u5de5\u4f5c\u7684\u6838\u51fd\u6570\u3002</li> <li>\u7531\u4e8e\u6838\u51fd\u6570\u4e0e\u5176\u4ed6\u6b63\u5728\u8fd0\u884c\u7684\u6838\u51fd\u6570\u65e0\u5173\uff0c\u56e0\u6b64\u6267\u884c\u914d\u7f6e\u5fc5\u987b\u4f7f\u6838\u51fd\u6570\u6267\u884c\u6b63\u786e\u7684\u6b21\u6570\uff0c\u4f8b\u5982\u5faa\u73af\u8fed\u4ee3\u7684\u6b21\u6570\u3002\uff08\u6ce8\u610f\uff1a\u5404\u4e2a\u7ebf\u7a0b\u6267\u884c\u987a\u5e8f\u4e0d\u5b9a\uff0c\u6545\u53ef\u52a0\u901f\u7684\u5faa\u73af\u9700\u8981\u5404\u6b21\u8fed\u4ee3\u65e0\u5173\u8054\u4e14\u987a\u5e8f\u65e0\u5f71\u54cd\uff09</li> </ul> <p>\u4f8b\u5b50\uff1a</p> <p>\u52a0\u901f\u524d\uff1a</p> <pre><code>for (int i = 0; i &lt; N; ++i)\n{\nprintf(\"%d\\n\", i);\n}\n</code></pre> <p>\u52a0\u901f\u540e\uff1a\u4f7f\u7528\u6838\u51fd\u6570\u5b9e\u73b0\u5355\u8bcd\u8fed\u4ee3</p> <pre><code>__global__ void loop() {\nint idx = threadIdx.x + blockIdx.x * blockDim.x;\nif (idx &lt; N)\nprintf(\"%d\\n\", idx);\n}\n</code></pre> <p>\u5e76\u8c03\u7528</p> <pre><code>loop&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;();\n</code></pre> <p>\u6b64\u5904\u5fc5\u987b\u6709 <code>number_of_blocks * threads_per_block &gt;= N</code> \u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#_4","title":"\u5206\u914d\u5185\u5b58","text":"<p>\u56de\u5fc6\u4e00\u822c\u7684 CPU \u7a0b\u5e8f\u5206\u914d\u5e76\u91ca\u653e\u5185\u5b58\u7684\u65b9\u5f0f\uff1a</p> <pre><code>int *a;\na = (int *)malloc(size);\n...\nfree(a);\n</code></pre> <p>\u8981\u5206\u914d\u548c\u91ca\u653e\u5185\u5b58\uff0c\u5e76\u83b7\u53d6\u53ef\u5728\u4e3b\u673a\u548c\u8bbe\u5907\u4ee3\u7801\u4e2d\u5f15\u7528\u7684\u6307\u9488\uff0c\u5219\u9700\u8981\u4f7f\u7528 CUDA \u63d0\u4f9b\u7684\u51fd\u6570 <code>cudaMallocManaged()</code> \u548c <code>cudaFree()</code> \uff1a</p> <pre><code>int *a;\ncudaMallocManaged(&amp;a, size);\n...\ncudaFree(a);\n</code></pre> <p>\u5176\u4ed6\u4e00\u4e9b\u7528\u4e8e\u624b\u52a8\u5185\u5b58\u7ba1\u7406\u7684 CUDA \u547d\u4ee4\uff1a</p> <ul> <li><code>cudaMalloc</code> \u547d\u4ee4\u5c06\u76f4\u63a5\u4e3a\u5904\u4e8e\u6d3b\u52a8\u72b6\u6001\u7684 GPU \u5206\u914d\u5185\u5b58\u3002\u8fd9\u53ef\u9632\u6b62\u51fa\u73b0\u6240\u6709 GPU \u5206\u9875\u9519\u8bef\uff0c\u800c\u4ee3\u4ef7\u662f\u4e3b\u673a\u4ee3\u7801\u5c06\u65e0\u6cd5\u8bbf\u95ee\u8be5\u547d\u4ee4\u8fd4\u56de\u7684\u6307\u9488\u3002</li> <li><code>cudaMallocHost</code> \u547d\u4ee4\u5c06\u76f4\u63a5\u4e3a CPU \u5206\u914d\u5185\u5b58\u3002\u8be5\u547d\u4ee4\u53ef\u56fa\u5b9a\u5185\u5b58\uff08pinned memory\uff09\u6216\u9875\u9501\u5b9a\u5185\u5b58\uff08page-locked memory\uff09\u3002\u5141\u8bb8\u5c06\u5185\u5b58\u5f02\u6b65\u62f7\u8d1d\u81f3 GPU \u6216\u4ece GPU \u5f02\u6b65\u62f7\u8d1d\u81f3\u5185\u5b58\u3002\u4f46\u56fa\u5b9a\u5185\u5b58\u8fc7\u591a\u4f1a\u5e72\u6270 CPU \u6027\u80fd\uff0c\u56e0\u6b64\u9700\u907f\u514d\u65e0\u7aef\u4f7f\u7528\u8be5\u547d\u4ee4\u3002\u91ca\u653e\u56fa\u5b9a\u5185\u5b58\u65f6\u5e94\u4f7f\u7528 <code>cudaFreeHost</code> \u547d\u4ee4\u3002</li> <li><code>cudaMemcpy</code> \u547d\u4ee4\u53ef\u62f7\u8d1d\u5185\u5b58\u3002\u793a\u4f8b   <pre><code>// \u4ece host \u5411 device \u62f7\u8d1d\u5185\u5b58\ncudaMemcpy(device_a, host_a, size, cudaMemcpyHostToDevice);\n// \u4ece device \u5411 host \u62f7\u8d1d\u5185\u5b58\ncudaMemcpy(host_a, device_a, size, cudaMemcpyDeviceToHost);\n</code></pre></li> </ul>"},{"location":"HPC/HPC%20101/cudaC/#_5","title":"\u8de8\u7f51\u683c\u5faa\u73af","text":"<p>\u5f53\u6570\u636e\u96c6\u6bd4\u7f51\u683c\u5927\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u4f7f\u6bcf\u4e2a\u7ebf\u7a0b\u5904\u7406\u591a\u4e2a\u6570\u636e\uff1a</p> <pre><code>__global__ void kernel(int *a, int N)\n{\nint idx = threadIdx.x + blockIdx.x * blockDim.x;\nint stride = gridDim.x * blockDim.x;\n\nfor (int i = idx; i &lt; N; i += stride)\n{\n// do work on a[i];\n}\n}\n</code></pre>"},{"location":"HPC/HPC%20101/cudaC/#_6","title":"\u9519\u8bef\u5904\u7406","text":"<ul> <li>\u8bb8\u591a CUDA \u51fd\u6570\u4f1a\u8fd4\u56de\u7c7b\u578b\u4e3a <code>cudaError_t</code> \u7684\u503c\uff0c\u53ef\u7528\u4e8e\u68c0\u67e5\u8c03\u7528\u662f\u5426\u51fa\u9519\u3002</li> <li>\u4e3a\u68c0\u67e5\u542f\u52a8\u6838\u51fd\u6570\u65f6\u662f\u5426\u53d1\u751f\u9519\u8bef\uff0cCUDA \u63d0\u4f9b\u4e86 <code>cudaGetLastError</code> \u51fd\u6570\uff0c\u8fd4\u56de\u7c7b\u578b\u4e3a <code>cudaError_t</code> \u7684\u503c\u3002</li> <li>\u4e3a\u6355\u6349\u5f02\u6b65\u9519\u8bef\uff08\u4f8b\u5982\uff0c\u5728\u5f02\u6b65\u6838\u51fd\u6570\u6267\u884c\u671f\u95f4\uff09\uff0c\u9700\u8981\u68c0\u67e5\u540e\u7eed\u540c\u6b65 CUDA \u8fd0\u884c\u65f6 API \u8c03\u7528\u6240\u8fd4\u56de\u7684\u72b6\u6001\uff08\u4f8b\u5982 <code>cudaDeviceSynchronize</code> \uff09\uff0c\u5982\u679c\u4e4b\u524d\u542f\u52a8\u7684\u5176\u4e2d\u4e00\u4e2a\u6838\u51fd\u6570\u5931\u8d25\uff0c\u5219\u5c06\u8fd4\u56de\u9519\u8bef\u3002</li> </ul> <p>\u9519\u8bef\u5927\u81f4\u53ef\u5206\u4e3a\u540c\u6b65\u9519\u8bef <code>synError</code> \u548c\u5f02\u6b65\u9519\u8bef <code>asynError</code> \u3002</p> <p>\u5bf9\u4e8e <code>cudaError_t</code> \u503c\uff0c\u5c06\u5176\u4e0e CUDA \u63d0\u4f9b\u7684 <code>cudaSuccess</code> \u8fdb\u884c\u6bd4\u8f83\uff0c\u5373\u53ef\u5224\u65ad\u662f\u5426\u51fa\u9519\uff0c\u540c\u65f6\u53ef\u4ee5\u4f7f\u7528\u51fd\u6570 <code>cudaGetErrorString()</code> \u83b7\u5f97\u9519\u8bef\u4fe1\u606f\u3002</p> <p>\u4e00\u4e2a\u5305\u88c5 CUDA \u51fd\u6570\u8c03\u7528\u7684\u5b8f\uff1a</p> <pre><code>inline cudaError_t checkCuda(cudaError_t result)\n{\nif (result != cudaSuccess) {\nfprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\nassert(result == cudaSuccess);\n}\nreturn result;\n}\n\nint main()\n{\n\n/*\n * The macro can be wrapped around any function returning\n * a value of type `cudaError_t`.\n */\n\ncheckCuda( cudaDeviceSynchronize() )\n}\n</code></pre>"},{"location":"HPC/HPC%20101/cudaC/#-","title":"\u8fdb\u9636\u5185\u5bb9 - \u591a\u7ef4\u7f51\u683c\u548c\u5757","text":"<p>\u53ef\u4ee5\u5c06\u7f51\u683c\u548c\u7ebf\u7a0b\u5757\u5b9a\u4e49\u4e3a\u6700\u591a\u5177\u6709 3 \u4e2a\u7ef4\u5ea6\uff0c CUDA \u63d0\u4f9b <code>dim3</code> \u7c7b\u578b\u5b9a\u4e49\u591a\u7ef4\u7f51\u683c\u548c\u5757\uff1a</p> <pre><code>dim3 threads_per_block(block_dim_x, block_dim_y, block_dim_z);\ndim3 number_of_blocks(grid_dim_x, grid_dim_y, grid_dim_z);\nsomeKernel&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;();\n</code></pre> <p>\u5728\u6838\u51fd\u6570\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>threadIdx.y</code> \u53ca\u7c7b\u4f3c\u5f62\u5f0f\u83b7\u5f97\u76f8\u5173\u7d22\u5f15\u548c\u7ef4\u5ea6\u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#nsys","title":"\u7edf\u4e00\u5185\u5b58\u4f7f\u7528\u4e0e\u4f7f\u7528 nsys \u7ba1\u7406\u5185\u5b58","text":""},{"location":"HPC/HPC%20101/cudaC/#nsys_1","title":"nsys \u4f7f\u7528","text":"<p>\u4f7f\u7528 <code>nsys profile</code> \u5206\u6790\u7f16\u8bd1\u597d\u7684\u53ef\u6267\u884c\u6587\u4ef6</p> <pre><code>$ nsys profile --stats=true ./test\n</code></pre> <p><code>nsys profile</code> \u5c06\u751f\u6210\u4e00\u4e2a <code>qdrep</code> \u62a5\u544a\u6587\u4ef6\uff0c\u4f7f\u7528 <code>--stats = true</code> \u6807\u5fd7\u8868\u793a\u5e0c\u671b\u6253\u5370\u8f93\u51fa\u6458\u8981\u7edf\u8ba1\u4fe1\u606f\u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#streaming-multiprocessors","title":"\u6d41\u591a\u5904\u7406\u5668\uff08Streaming Multiprocessors\uff09","text":"<p>GPU \u5177\u6709\u79f0\u4e3a\u6d41\u591a\u5904\u7406\u5668\uff08\u6216 SM\uff09\u7684\u5904\u7406\u5355\u5143\u3002\u5728\u6838\u51fd\u6570\u6267\u884c\u671f\u95f4\uff0c\u5c06\u7ebf\u7a0b\u5757\u63d0\u4f9b\u7ed9 SM \u4ee5\u4f9b\u5176\u6267\u884c\u3002SM \u540c\u65f6\u8c03\u5ea6\u6267\u884c\u7684\u7ebf\u7a0b\u5757\u53d6\u51b3\u4e8e warp \u5927\u5c0f\uff08\u4e00\u822c\u4e3a 32\uff09\u3002</p> <p>\u901a\u5e38\u53ef\u4ee5\u9009\u62e9\u7ebf\u7a0b\u6570\u91cf\u6570\u500d\u4e8e 32 \u7684\u7ebf\u7a0b\u5757\u5927\u5c0f\u6765\u63d0\u5347\u6027\u80fd\u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#gpu_3","title":"\u67e5\u8be2 GPU \u5c5e\u6027","text":"<pre><code>int deviceId;\ncudaGetDevice(&amp;deviceId);\n\ncudaDeviceProp props;\ncudaGetDeviceProperties(&amp;props, deviceId);\n</code></pre> <p><code>props</code> \u4e2d\u5305\u542b\u4e86 GPU \u8bbe\u5907\u5c5e\u6027\uff0c\u4e3b\u8981\u7528\u5230 warp \u5927\u5c0f <code>props.warpSize</code> \u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#um","title":"\u7edf\u4e00\u5185\u5b58\uff08UM\uff09\u884c\u4e3a","text":"<p>\u5206\u914d UM \u65f6\uff0c\u5185\u5b58\u5c1a\u672a\u9a7b\u7559\u5728\u4e3b\u673a\u6216\u8bbe\u5907\u4e0a\u3002\u4e3b\u673a\u6216\u8bbe\u5907\u5c1d\u8bd5\u8bbf\u95ee\u5185\u5b58\u65f6\u4f1a\u53d1\u751f\u9875\u9519\u8bef\uff08Page Fault\uff09\uff0c\u6b64\u65f6\u4e3b\u673a\u6216\u8bbe\u5907\u4f1a\u6279\u91cf\u8fc1\u79fb\u6240\u9700\u7684\u6570\u636e\u3002\u540c\u7406\uff0c\u5f53 CPU \u6216\u52a0\u901f\u7cfb\u7edf\u4e2d\u7684\u4efb\u4f55 GPU \u5c1d\u8bd5\u8bbf\u95ee\u5c1a\u672a\u9a7b\u7559\u5728\u5176\u4e0a\u7684\u5185\u5b58\u65f6\uff0c\u4f1a\u53d1\u751f\u9875\u9519\u8bef\u5e76\u89e6\u53d1\u8fc1\u79fb\u3002</p> <p>\u7a00\u758f\u8bbf\u95ee\u6570\u636e\u65f6\uff0c\u89e6\u53d1\u9875\u9519\u8bef\u5e76\u6309\u9700\u8fc1\u79fb\u5185\u5b58\u4f1a\u6709\u663e\u8457\u4f18\u52bf\u3002</p> <p>\u800c\u9700\u8981\u5927\u91cf\u8fde\u7eed\u7684\u5185\u5b58\u5757\u65f6\uff0c\u901a\u8fc7\u5f02\u6b65\u9884\u53d6\u5185\u5b58\u53ef\u4ee5\u6709\u6548\u89c4\u907f\u9875\u9519\u8bef\u548c\u6309\u9700\u6570\u636e\u8fc1\u79fb\u6240\u4ea7\u751f\u7684\u5f00\u9500\u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#_7","title":"\u5f02\u6b65\u5185\u5b58\u9884\u53d6","text":"<p>CUDA \u53ef\u901a\u8fc7 <code>cudaMemPrefetchAsync</code> \u51fd\u6570\uff0c\u5c06\u6258\u7ba1\u5185\u5b58\u5f02\u6b65\u9884\u53d6\u5230 GPU \u8bbe\u5907\u6216 CPU\uff1a</p> <pre><code>int deviceId;\ncudaGetDevice(&amp;deviceId);\n\n// Prefetch to GPU device.\ncudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);\n// Prefetch to host.\ncudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); </code></pre>"},{"location":"HPC/HPC%20101/cudaC/#cuda_1","title":"\u5e76\u53d1 CUDA \u6d41","text":"<p>\u6d41\u662f\u7531\u6309\u987a\u5e8f\u6267\u884c\u7684\u4e00\u7cfb\u5217\u547d\u4ee4\u6784\u6210\u3002\u5728 CUDA \u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u6838\u51fd\u6570\u7684\u6267\u884c\u4ee5\u53ca\u4e00\u4e9b\u5185\u5b58\u4f20\u8f93\u5747\u5728 CUDA \u6d41\u4e2d\u8fdb\u884c\u3002\u672a\u4f5c\u7279\u6b8a\u58f0\u660e\u7684\u6838\u51fd\u6570\u5728\u9ed8\u8ba4\u6d41\u4e2d\u6267\u884c\u3002</p> <p>\u6b64\u5916\uff0c\u7a0b\u5e8f\u53ef\u4ee5\u521b\u5efa\u975e\u9ed8\u8ba4\u6d41\uff0c\u5728\u4e0d\u540c\u7684\u6d41\u4e2d\u5e76\u53d1\u6267\u884c\u591a\u4e2a\u6838\u51fd\u6570\u3002</p> <p>\u9ed8\u8ba4\u6d41\u4f1a\u963b\u6b62\u5176\u4ed6\u6d41\u4e2d\u7684\u6240\u6709\u6838\u51fd\u6570\u3002\u5f53\u5176\u4ed6\u6d41\u4e2d\u7684\u6240\u6709\u6838\u51fd\u6570\u6267\u884c\u5b8c\u6bd5\u4e4b\u540e\uff0c\u9ed8\u8ba4\u6d41\u4e2d\u7684\u6838\u51fd\u6570\u624d\u5f00\u59cb\u6267\u884c\uff1b\u5f53\u9ed8\u8ba4\u6d41\u4e2d\u7684\u6838\u51fd\u6570\u6267\u884c\u5b8c\u6bd5\uff0c\u5176\u4ed6\u6d41\u4e2d\u7684\u6838\u51fd\u6570\u624d\u53ef\u4ee5\u5f00\u59cb\u6267\u884c\u3002</p> <p></p> <p>\u521b\u5efa\u975e\u9ed8\u8ba4\u6d41\uff0c\u5e76\u5728\u975e\u9ed8\u8ba4\u6d41\u4e2d\u542f\u52a8\u6838\u51fd\u6570\uff1a</p> <pre><code>cudaStream_t stream;\ncudaStreamCreate(&amp;stream);\n\nsomeKernel&lt;&lt;&lt;number_of_blocks, threads_per_block, 0, stream&gt;&gt;&gt;();\n\ncudaStreamDestroy(stream);\n</code></pre>"},{"location":"Labs/numpy/","title":"lab2 \u5411\u91cf\u5316\u8ba1\u7b97","text":""},{"location":"Labs/numpy/#_1","title":"\u601d\u8def","text":""},{"location":"Labs/numpy/#step-1","title":"Step 1","text":"<p>\u5148\u8003\u8651\u8ba1\u7b97\u5355\u5f20\u56fe\u7247\u7684\u60c5\u5f62\uff08 <code>N = 1</code> \uff09\u3002</p> <p>\u4e0b\u8bbe\u65b0\u7684 \\(H_2 \\times W_2\\) \u7684\u56fe\u4e2d \\((i,j)\\) \u5904\u50cf\u7d20\u6240\u60f3\u8981\u91c7\u6837\u7684 <code>a</code> \u56fe\u4e2d\u5bf9\u5e94\u70b9\u5750\u6807\u4e3a \\((x_{(i,j)}, y_{(i,j)})\\) \uff0c\u5373 <code>b[n, i, j]</code> \u5904\u6240\u5b58\u5750\u6807\u3002</p> <p>\u9605\u8bfb <code>baseline.py</code> \uff0c\u53ef\u77e5\u57fa\u51c6\u4ee3\u7801\u91c7\u7528\u4e86\u5982\u4e0b\u7b49\u5f0f\u8ba1\u7b97\u6bcf\u4e2a\u91c7\u6837\u70b9 <code>(x, y)</code> \u7684\u91c7\u6837\u7ed3\u679c\uff1a</p> \\[ \\begin{aligned}     f(x,y) \\approx     {\\frac {1}{(x_{2}-x_{1})(y_{2}-y_{1})}}      {\\big (}     &amp;f(Q_{11})(x_{2}-x)(y_{2}-y) + f(Q_{21})(x-x_{1})(y_{2}-y)\\\\     +&amp;f(Q_{12})(x_{2}-x)(y-y_{1}) + f(Q_{22})(x-x_{1})(y-y_{1})     {\\big )} \\end{aligned} \\] <p>\u5728\u672c\u6b21\u5b9e\u9a8c\u7ed9\u51fa\u7684\u60c5\u666f\u4e2d\uff0c \\(x_2 - x_1 = y_2 - y_1 = 1, x_1 = \\lfloor x \\rfloor, y_1 = \\lfloor y \\rfloor\\)  \uff0c\u8bb0  \\(x' = x - \\lfloor x \\rfloor\uff0c y' = y - \\lfloor y \\rfloor\\)  \uff0c\u4e0a\u8ff0\u7b49\u5f0f\u53ef\u7b80\u5316\u4e3a\uff1a</p> \\[ \\displaystyle \\begin{aligned}     f(x,y) \\approx      \\ &amp; f(Q_{11})(1 - x')(1 - y')+f(Q_{21})(x')(1 - y') \\\\     + \\ &amp; f(Q_{12})(1 - x')(y')+f(Q_{22})(x')(y') \\\\ \\end{aligned} \\] <p>\u7531\u4e8e\u6bcf\u4e2a\u91c7\u6837\u70b9\u7684\u8ba1\u7b97\u90fd\u662f\u72ec\u7acb\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49\u5982\u4e0b\u51e0\u4e2a\u77e9\u9635\uff0c</p> \\[ \\mathbf{F} = \\big( f(x_{(i,j)}, y_{(i,j)})\\big)_{H2 \\times W2}  \\] \\[ \\begin{aligned}     \\mathbf{F_{00}} = \\big( f(Q_{11})\\big)_{H2 \\times W2} \\quad     \\mathbf{F_{01}} = \\big( f(Q_{12})\\big)_{H2 \\times W2} \\\\     \\mathbf{F_{10}} = \\big( f(Q_{21})\\big)_{H2 \\times W2} \\quad     \\mathbf{F_{11}} = \\big( f(Q_{22})\\big)_{H2 \\times W2} \\\\ \\end{aligned} \\] \\[ \\begin{aligned}     \\mathbf{X'} = \\big( x'_{(i, j)} \\big)_{H2 \\times W2} \\\\     \\mathbf{Y'} = \\big( y'_{(i, j)} \\big)_{H2 \\times W2} \\end{aligned} \\] <p>\u4f7f\u7528\u9010\u5143\u7d20\u4e58\u6cd5\uff08 <code>*</code> \uff09\u548c\u77e9\u9635\u52a0\u6cd5\u5373\u53ef\u5b9e\u73b0\u5411\u91cf\u5316\u8ba1\u7b97\u77e9\u9635 \\(\\mathbf{F}\\) \u4e2d\u7684\u91c7\u6837\u4fe1\u606f\uff1a</p> \\[ \\begin{aligned}     \\mathbf{F}     &amp; = \\mathbf{F_{00}} * (1 - \\mathbf{X'}) * (1 - \\mathbf{Y'}) + \\mathbf{F_{10}} * \\mathbf{X'} * (1 - \\mathbf{Y'}) \\\\     &amp; + \\mathbf{F_{01}} * (1 - \\mathbf{X'}) * \\mathbf{Y'} + \\mathbf{F_{11}} * \\mathbf{X'} * \\mathbf{Y'}  \\end{aligned} \\] <p>\u636e\u6b64\u5373\u53ef\u4f7f\u7528\u5411\u91cf\u5316\u53bb\u6389\u5185\u5c42\u4e24\u5c42 <code>for</code> \u5faa\u73af\uff0c\u4ee3\u7801\u5b9e\u73b0\u5982\u4e0b\uff1a</p> vectorize1.py<pre><code>def bilinear_interp_vectorized(a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    This is the vectorized implementation of bilinear interpolation.\n    - a is a ND array with shape [N, H1, W1, C], dtype = int64\n    - b is a ND array with shape [N, H2, W2, 2], dtype = float64\n    - return a ND array with shape [N, H2, W2, C], dtype = int64\n    \"\"\"\n    # get axis size from ndarray shape\n    N, H1, W1, C = a.shape\n    N1, H2, W2, _ = b.shape\n    assert N == N1\n\n    # Do iteration\n    b = b.transpose(0, 3, 1, 2)\n    res = np.empty((N, H2, W2, C), dtype=int64)\n\n    for n in range(N):\n        X, Y = b[n]\n        X_idx , Y_idx = np.floor(X).astype(int64), np.floor(Y).astype(int64)\n        _X, _Y = X - X_idx, Y - Y_idx\n        A00 = a[n, X_idx, Y_idx].transpose(2, 0, 1)\n        A01 = a[n, X_idx, Y_idx + 1].transpose(2, 0, 1)\n        A10 = a[n, X_idx + 1, Y_idx].transpose(2, 0, 1)\n        A11 = a[n, X_idx + 1, Y_idx + 1].transpose(2, 0, 1)\n        res00 = A00 * (1 - _X) * (1 - _Y)\n        res10 = A10 * _X * (1 - _Y)\n        res01 = A01 * (1 - _X) * _Y\n        res11 = A11 * _X * _Y\n        res[n] = res00.transpose(1, 2, 0) + res01.transpose(1, 2, 0) + \\\n                 res10.transpose(1, 2, 0) + res11.transpose(1, 2, 0)\n    return res\n</code></pre> <p>Note</p> <p>\u4ee3\u7801\u4e2d\u4f7f\u7528\u4e86 <code>np.transpose</code> \u51fd\u6570\u8fdb\u884c\u77e9\u9635\u8f6c\u7f6e\uff0c\u4f7f\u5f97 <code>X, Y</code> \u80fd\u5bb9\u6613\u5730\u4ece <code>b</code> \u4e2d\u53d6\u51fa\uff0c\u5e76\u4e14\u4f7f\u5f97\u5728\u8fdb\u884c\u77e9\u9635\u9010\u5143\u7d20\u4e58\u6cd5\u65f6\u80fd\u6ee1\u8db3\u5e7f\u64ad\u89c4\u5219\uff0c\u540c\u65f6\u5bf9 <code>C</code> \u4e2a\u901a\u9053\u7684\u4fe1\u606f\u8fdb\u884c\u5904\u7406\u3002</p>"},{"location":"Labs/numpy/#step-2","title":"Step 2","text":"<p>\u73b0\u5728\u8003\u8651\u540c\u65f6\u5bf9 <code>N</code> \u5f20\u56fe\u7247\u8fdb\u884c\u5904\u7406\u3002</p> <p>\u5bf9 <code>vectorize1.py</code> \u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u4fee\u6539\u3002</p> <p>\u6211\u4eec\u8981\u5c06 <code>N</code> \u5f20\u56fe\u7247\u7684\u4fe1\u606f\u8fdb\u884c\u6574\u5408\u5904\u7406\uff0c\u4e00\u4e2a\u53ef\u884c\u7684\u65b9\u6cd5\u4e3a\u5728 <code>X, Y</code> \u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e00\u4e2a\u4ee3\u8868\u56fe\u7247\u7d22\u5f15\u7684\u8f74\uff0c\u4f7f\u5176\u5f62\u72b6\u4ece <code>(H2, W2)</code> \u53d8\u4e3a <code>(N, H2, W2)</code> \u3002\u5177\u4f53\u5b9e\u73b0\u5982\u4e0b\uff08 <code>X_idx, Y_idx, _X, _Y</code> \u7684\u83b7\u53d6\u4e00\u5e76\u7ed9\u51fa\uff09\uff1a</p> <pre><code>X, Y = b.transpose(3, 0, 1, 2)\nX_idx , Y_idx = np.floor(X).astype(int64), np.floor(Y).astype(int64)\n_X, _Y = X - X_idx, Y - Y_idx\n</code></pre> <p>\u540c\u7406\uff0c<code>A00, A01, A10, A11</code> \u5747\u9700\u8981\u589e\u52a0\u4e00\u4e2a\u8f74\uff0c\u4e8e\u662f\u65b0\u589e\u4e00\u4e2a\u7d22\u5f15\u6570\u7ec4 <code>N_idx</code> \u5bf9 <code>a</code> \u6570\u7ec4\u7684\u7b2c\u4e00\u4e2a\u8f74 \uff08 <code>axis = 0</code> \uff09 \u8fdb\u884c\u7d22\u5f15\u3002\u5177\u4f53\u5b9e\u73b0\u5982\u4e0b\uff1a</p> <pre><code>N_idx = np.arange(N).reshape(N, 1, 1)\n\nA00 = a[N_idx, X_idx, Y_idx].transpose(3, 0, 1, 2)\nA01 = a[N_idx, X_idx, Y_idx + 1].transpose(3, 0, 1, 2)\nA10 = a[N_idx, X_idx + 1, Y_idx].transpose(3, 0, 1, 2)\nA11 = a[N_idx, X_idx + 1, Y_idx + 1].transpose(3, 0, 1, 2)\n</code></pre> <p>Note</p> <p><code>N_idx</code> \u7684\u5f62\u72b6\u4e0e <code>X_idx, Y_idx</code> \u5e76\u4e0d\u5339\u914d\uff0c\u4e4b\u6240\u4ee5\u80fd\u591f\u50cf\u8fd9\u6837\u5bf9 <code>a</code> \u6570\u7ec4\u8fdb\u884c fancy indexing \uff0c\u662f\u56e0\u4e3a\u8fdb\u884c\u7d22\u5f15\u7684\u65f6\u5019\u5bf9 <code>N_idx</code> \u7684\u540e\u4e24\u4e2a\u8f74\uff08 <code>axis=1, axis=2</code> \uff09\u8fdb\u884c\u4e86 broadcast \u3002</p> <p>\u5b8c\u6574\u4ee3\u7801\u5982\u4e0b\uff1a</p> vectorize.py<pre><code>def bilinear_interp_vectorized(a: np.ndarray, b: np.ndarray) -&gt; np.ndarray:\n\"\"\"\n    This is the vectorized implementation of bilinear interpolation.\n    - a is a ND array with shape [N, H1, W1, C], dtype = int64\n    - b is a ND array with shape [N, H2, W2, 2], dtype = float64\n    - return a ND array with shape [N, H2, W2, C], dtype = int64\n    \"\"\"\n    # get axis size from ndarray shape\n    N, _, _, C = a.shape\n    N1, H2, W2, _ = b.shape\n    assert N == N1\n\n    # Do iteration\n    res = np.empty((N, H2, W2, C), dtype=int64)\n\n    # Get the matrices of coordinates\n    X, Y = b.transpose(3, 0, 1, 2)\n    X_idx , Y_idx = np.floor(X).astype(int64), np.floor(Y).astype(int64)\n    _X, _Y = X - X_idx, Y - Y_idx\n\n    # Generate the indices array\n    N_idx = np.arange(N).reshape(N, 1, 1)\n\n    A00 = a[N_idx, X_idx, Y_idx].transpose(3, 0, 1, 2)\n    A01 = a[N_idx, X_idx, Y_idx + 1].transpose(3, 0, 1, 2)\n    A10 = a[N_idx, X_idx + 1, Y_idx].transpose(3, 0, 1, 2)\n    A11 = a[N_idx, X_idx + 1, Y_idx + 1].transpose(3, 0, 1, 2)\n    res = A00 * (1 - _X) * (1 - _Y) + A01 * (1 - _X) * _Y + \\\n          A10 * _X * (1 - _Y) + A11 * _X * _Y\n    return res.transpose(1, 2, 3, 0).astype(int64)\n</code></pre>"},{"location":"Labs/numpy/#_2","title":"\u6b63\u786e\u6027\u4e0e\u52a0\u901f\u6bd4","text":"<p>\u8fd0\u884c <code>main.py</code> \uff0c\u67e5\u770b\u8f93\u51fa\uff1a</p> <pre><code>Generating Data...\nExecuting Baseline Implementation...\nFinished in 99.58129525184631s\nExecuting Vectorized Implementation...\nFinished in 3.138871192932129s\n[PASSED] Results are identical.\nSpeed Up 31.725193272051396x\n</code></pre> <p>\u53ef\u89c1\u8fd0\u884c\u7ed3\u679c\u6b63\u786e\uff0c\u52a0\u901f\u6bd4\u4e3a 31.725193272051396 \u3002</p>"},{"location":"Labs/numpy/#reference","title":"Reference","text":"<ul> <li>\u521d\u63a2Numpy\u4e2d\u7684\u82b1\u5f0f\u7d22\u5f15\uff1ahttps://zhuanlan.zhihu.com/p/123858781</li> <li>Numpy\u4e2dtranspose()\u51fd\u6570\u7684\u53ef\u89c6\u5316\u7406\u89e3\uff1ahttps://zhuanlan.zhihu.com/p/61203757</li> <li>NumPy \u4e2d\u6587\u6587\u6863\uff0c\u5e7f\u64ad\uff08Broadcasting\uff09\uff1ahttps://numpy.org.cn/user/basics/broadcasting.html</li> </ul>"},{"location":"Labs/simd/","title":"lab2.5 \u624b\u5199 SIMD \u5411\u91cf\u5316","text":""},{"location":"Labs/simd/#_1","title":"\u601d\u8def","text":"<p>\u9996\u5148\u5206\u6790\u9700\u8981\u5411\u91cf\u5316\u7684\u90e8\u5206\uff1a</p> <pre><code>for (int i = 0; i &lt; MAXN; ++i)\n{\nc[i] += a[i] * b[i];\n}\n</code></pre> <p>\u53ef\u4ee5\u5206\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a</p> <ul> <li>\u9700\u8981\u8ba1\u7b97\u7684\u6570\u636e\uff1a <code>a[i], b[i], c[i]</code> \u3002</li> <li>\u8ba1\u7b97\u64cd\u4f5c\uff1a <code>tmp = a[i] * b[i]</code> \u548c <code>c[i] = c[i] + tmp</code> \u3002</li> </ul> <p>\u4e8e\u662f\u6839\u636e\u5411\u91cf\u5316\u7684\u57fa\u672c\u6d41\u7a0b\uff0c\u5927\u81f4\u9700\u8981\u51e0\u4e2a\u64cd\u4f5c\uff1a</p> <ul> <li>Load <code>a, b, c</code> \u5230 <code>__m256</code> \u7c7b\u578b\u53d8\u91cf <code>A, B, C</code> \u3002</li> <li>\u4ee4 <code>C = C + A * B</code> \u3002\uff08\u6b64\u5904\u8fd0\u7b97\u7b26\u53ea\u4ee3\u8868\u64cd\u4f5c\u7684\u542b\u4e49\uff09</li> <li>\u5c06 <code>C</code> \u7684\u6570\u636e\u5b58\u56de <code>float</code> \u6570\u7ec4 <code>c</code> \u4e2d \u3002</li> </ul> <p>\u5982\u4f55\u5b9e\u73b0\uff1f</p> <p>Load \u64cd\u4f5c\uff1a</p> <code>__m256 _mm256_loadu_ps (float const * mem_addr)</code> <p>\u4ece <code>mem_addr</code> \u6307\u5411\u7684\u5185\u5b58\u5730\u5740\u4e2d load 256 \u4f4d\uff088 \u4e2a <code>float</code> \u5143\u7d20\uff09\u7684\u6570\u636e\u5230\u5411\u91cf\u5bc4\u5b58\u5668\u3002</p> <p>\u8ba1\u7b97\u64cd\u4f5c\uff1a</p> <code>__m256 _mm256_mul_ps (__m256 a, __m256 b)</code> <p>\u8ba1\u7b97 <code>a</code> \u548c <code>b</code> \u4e2d <code>float</code> \u5143\u7d20\u9010\u4f4d\u76f8\u4e58\u7684\u7ed3\u679c\u3002</p> <code>__m256 _mm256_add_ps (__m256 a, __m256 b)</code> <p>\u8ba1\u7b97 <code>a</code> \u548c <code>b</code> \u4e2d <code>float</code> \u5143\u7d20\u9010\u4f4d\u76f8\u52a0\u7684\u7ed3\u679c\u3002</p> <p>Store \u64cd\u4f5c\uff1a</p> <code>void _mm256_storeu_ps (float * mem_addr, __m256 a)</code> <p>\u5c06 <code>a</code> \u4e2d\u7684 256 \u4f4d\uff088 \u4e2a <code>float</code> \u5143\u7d20\uff09\u7684\u6570\u636e store \u5230 <code>mem_addr</code> \u6307\u5411\u7684\u5185\u5b58\u5730\u5740\u4e2d\u3002</p> <p>\u4e8e\u662f\u5c31\u53ef\u4ee5\u5199\u51fa\u5411\u91cf\u5316\u7684\u4ee3\u7801\u4e86\uff1a</p> <pre><code>#define LEN (MAXN / 8)\nfor (int i = 0; i &lt; LEN; ++i) {\n__m256 A, B, C;\n\n// Load data\nA = _mm256_loadu_ps(a + i * 8);\nB = _mm256_loadu_ps(b + i * 8);\nC = _mm256_loadu_ps(c + i * 8);\n\n// Calculate\nC = _mm256_add_ps(C, _mm256_mul_ps(A, B));\n\n// Store data\n_mm256_storeu_ps(c + i * 8, C);\n}\n</code></pre> <p>Note</p> <p>\u7531\u4e8e <code>__m256</code> \u7c7b\u578b\u53d8\u91cf\u53ea\u80fd\u540c\u65f6\u64cd\u4f5c 256 \u4f4d\u6570\u636e\uff0c\u5373 8 \u4e2a <code>float</code> \uff0c\u5411\u91cf\u5316\u65f6\u9700\u5bf9\u6bcf 8 \u4e2a\u6570\u636e\u8fdb\u884c\u4e00\u6b21\u5411\u91cf\u5316</p>"},{"location":"Labs/simd/#_2","title":"\u6b63\u786e\u6027\u548c\u52a0\u901f\u6bd4","text":"<p>\u7f16\u8bd1\u8fd0\u884c <code>add.cpp</code> \uff0c\u7a0b\u5e8f\u8f93\u51fa\uff1a</p> <pre><code>time=1.916000\nCheck Passed\n</code></pre> <p>\u5411\u91cf\u5316\u540e\u8ba1\u7b97\u7ed3\u679c\u6b63\u786e\uff0c\u52a0\u901f\u6bd4\u4e3a 1.916 \u3002</p>"},{"location":"Labs/simd/#_3","title":"\u6c47\u7f16\u4ee3\u7801\u5206\u6790","text":"<p>\u5229\u7528 godbolt \u8fdb\u884c\u6c47\u7f16\u4ee3\u7801\u5206\u6790\u3002</p> <p>\u5411\u91cf\u5316\u524d\uff0c\u9700\u8981\u5411\u91cf\u5316\u7684\u4ee3\u7801\u90e8\u5206\u7684\u6c47\u7f16\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>...\nmov     DWORD PTR [rbp-20], 0\njmp     .L11\n.L12:\nmov     eax, DWORD PTR [rbp-20]\ncdqe\nvmovss  xmm1, DWORD PTR c[0+rax*4]\nmov     eax, DWORD PTR [rbp-20]\ncdqe\nvmovss  xmm2, DWORD PTR a[0+rax*4]\nmov     eax, DWORD PTR [rbp-20]\ncdqe\nvmovss  xmm0, DWORD PTR b[0+rax*4]\nvmulss  xmm0, xmm2, xmm0\nvaddss  xmm0, xmm1, xmm0\nmov     eax, DWORD PTR [rbp-20]\ncdqe\nvmovss  DWORD PTR c[0+rax*4], xmm0\nadd     DWORD PTR [rbp-20], 1\n.L11:\ncmp     DWORD PTR [rbp-20], 99999999\njle     .L12\n...\n</code></pre> <p>\u53ef\u77e5\uff0c<code>for</code> \u5faa\u73af\u5185\u90e8\u7684\u4ee3\u7801\u88ab\u987a\u6b21\u6267\u884c\u4e86 <code>MAXN</code> \u6b21\uff0c\u6bcf\u6b21\u53ea\u5bf9 1 \u7ec4\u6570\u636e\uff08 <code>a[i], b[i], c[i]</code> \uff09 \u8fdb\u884c\u64cd\u4f5c\u3002</p> <p>\u5411\u91cf\u5316\u540e\uff0c\u88ab\u5411\u91cf\u5316\u7684\u4ee3\u7801\u90e8\u5206\u7684\u6c47\u7f16\u4ee3\u7801\u5982\u4e0b\uff1a</p> <pre><code>...\nmov     DWORD PTR [rbp-36], 0\njmp     .L11\n.L17:\nmov     eax, DWORD PTR [rbp-36]\nsal     eax, 3\ncdqe\nsal     rax, 2\nadd     rax, OFFSET FLAT:a\nmov     QWORD PTR [rbp-392], rax\nmov     rax, QWORD PTR [rbp-392]\nvmovups ymm0, YMMWORD PTR [rax]\nvmovaps YMMWORD PTR [rbp-112], ymm0\nmov     eax, DWORD PTR [rbp-36]\nsal     eax, 3\ncdqe\nsal     rax, 2\nadd     rax, OFFSET FLAT:b\nmov     QWORD PTR [rbp-384], rax\nmov     rax, QWORD PTR [rbp-384]\nvmovups ymm0, YMMWORD PTR [rax]\nvmovaps YMMWORD PTR [rbp-144], ymm0\nmov     eax, DWORD PTR [rbp-36]\nsal     eax, 3\ncdqe\nsal     rax, 2\nadd     rax, OFFSET FLAT:c\nmov     QWORD PTR [rbp-376], rax\nmov     rax, QWORD PTR [rbp-376]\nvmovups ymm0, YMMWORD PTR [rax]\nvmovaps YMMWORD PTR [rbp-176], ymm0\nvmovaps ymm0, YMMWORD PTR [rbp-112]\nvmovaps YMMWORD PTR [rbp-336], ymm0\nvmovaps ymm0, YMMWORD PTR [rbp-144]\nvmovaps YMMWORD PTR [rbp-368], ymm0\nvmovaps ymm0, YMMWORD PTR [rbp-336]\nvmulps  ymm0, ymm0, YMMWORD PTR [rbp-368]\nvmovaps ymm1, YMMWORD PTR [rbp-176]\nvmovaps YMMWORD PTR [rbp-272], ymm1\nvmovaps YMMWORD PTR [rbp-304], ymm0\nvmovaps ymm0, YMMWORD PTR [rbp-272]\nvaddps  ymm0, ymm0, YMMWORD PTR [rbp-304]\nvmovaps YMMWORD PTR [rbp-176], ymm0\nmov     eax, DWORD PTR [rbp-36]\nsal     eax, 3\ncdqe\nsal     rax, 2\nadd     rax, OFFSET FLAT:c\nmov     QWORD PTR [rbp-184], rax\nvmovaps ymm0, YMMWORD PTR [rbp-176]\nvmovaps YMMWORD PTR [rbp-240], ymm0\nvmovaps ymm0, YMMWORD PTR [rbp-240]\nmov     rax, QWORD PTR [rbp-184]\nvmovups YMMWORD PTR [rax], ymm0\nnop\nadd     DWORD PTR [rbp-36], 1\n.L11:\ncmp     DWORD PTR [rbp-36], 12499999\njle     .L17\n...\n</code></pre> <p>\u53ef\u77e5\u7a0b\u5e8f\u4f7f\u7528\u4e86 <code>vmoups</code> \u7b49\u6c47\u7f16\u6307\u4ee4\u548c 256 \u4f4d\u5bc4\u5b58\u5668 <code>ymm0, ymm1</code> \uff0c\u540c\u65f6\u5bf9 8 \u7ec4 <code>float</code> \u7c7b\u578b\u6570\u636e\u8fdb\u884c\u64cd\u4f5c\uff0c\u5faa\u73af\u6b21\u6570\u51cf\u5c11\u5230 <code>MAXN / 8</code> \u6b21\u3002</p>"},{"location":"Labs/simd/#reference","title":"Reference","text":"<ul> <li>Intel\u00ae Intrinsics Guide\uff1ahttps://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html</li> <li>n\u65b9\u8fc7\u767e\u4e07 \u66b4\u529b\u78be\u6807\u7b97\u2014\u2014\u6307\u4ee4\u96c6\u4f18\u5316\u7684\u57fa\u7840\u4f7f\u7528\uff1ahttps://www.luogu.com.cn/blog/ouuan/avx-optimize</li> <li>\u6c47\u7f16\u8bed\u8a00\u7b14\u8bb0\uff08\u516d\uff09\u2014\u2014SIMD\u6307\u4ee4\uff1ahttps://zhuanlan.zhihu.com/p/424475308</li> </ul>"}]}