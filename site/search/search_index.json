{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"Welcome to My Notebook","text":"<p>\u8fd9\u91cc\u662f GoPoux \u7684\u4e2a\u4eba\u7b14\u8bb0\u672c</p>"},{"location":"CS/","title":"Computer Science","text":"<p>Abstract</p> <p>\u5f52\u6863\u5927\u5b66\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u6280\u672f\u8bfe\u7a0b\u7684\u4e00\u4e9b\u5b66\u4e60\u7b14\u8bb0</p>"},{"location":"CS/content/","title":"Content","text":""},{"location":"HPC/","title":"HPC","text":"<p>Abstract</p> <p>\u5f52\u6863 HPC \u7684\u4e00\u4e9b\u5b66\u4e60\u7b14\u8bb0</p>"},{"location":"HPC/HPC%20101/","title":"HPC 101","text":"<p>Abstract</p> <p>\u5927\u4e00\u6691\u5047\u8d85\u7b97\u77ed\u5b66\u671f labs</p> <p>\u5b9e\u9a8c\u624b\u518c</p>"},{"location":"HPC/HPC%20101/cudaC/","title":"NVIDIA CUDA C","text":"<p>Abstract</p> <p>GPU \u7f16\u7a0b\u539f\u7406\u4e0e NVIDIA CUDA C \u7f16\u7a0b\u5b9e\u9a8c</p>"},{"location":"HPC/HPC%20101/cudaC/#gpu","title":"GPU \u7f16\u7a0b\u539f\u7406","text":"<p>TODO:</p>"},{"location":"HPC/HPC%20101/cudaC/#cuda-cc","title":"CUDA C/C++ \u7f16\u7a0b","text":"<p>\u7531 NVIDIA \u6df1\u5ea6\u5b66\u4e60\u57f9\u8bad\u4e2d\u5fc3 (DLI) \u63d0\u4f9b\u7684\u8bfe\u7a0b\uff0c\u5728\u7531 NVIDIA \u63d0\u4f9b\u7684\u4e91\u73af\u5883\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#cuda-cc_1","title":"\u4f7f\u7528 CUDA C/C++ \u52a0\u901f\u7a0b\u5e8f","text":""},{"location":"HPC/HPC%20101/cudaC/#_1","title":"\u52a0\u901f\u7cfb\u7edf","text":"<p>\u52a0\u901f\u7cfb\u7edf\u53c8\u79f0\u5f02\u6784\u7cfb\u7edf\uff0c\u7531 CPU \u548c GPU \u7ec4\u6210\u3002\u52a0\u901f\u7cfb\u7edf\u8fd0\u884c CPU \u7a0b\u5e8f\uff0c\u7531\u8fd9\u4e9b\u7a0b\u5e8f\u8c03\u5ea6\u8fd0\u884c\u4e8e GPU \u4e0a\u7684\u51fd\u6570\uff0c\u901a\u8fc7 GPU \u5b9e\u73b0\u51fd\u6570\u7684\u5e76\u884c\u8ba1\u7b97\u3002</p> <p>\u67e5\u8be2 GPU \u4fe1\u606f\u547d\u4ee4\uff1a</p> <pre><code>nvidia-smi\n</code></pre>"},{"location":"HPC/HPC%20101/cudaC/#gpu_1","title":"GPU \u52a0\u901f\u539f\u7406","text":"<p>\u4e00\u4e2a\u7b80\u5355\u7684 GPU \u52a0\u901f\u5e94\u7528\u7a0b\u5e8f\u6267\u884c\u8fc7\u7a0b\uff1a</p> <p></p> <ul> <li><code>(1)</code> \u6bb5\uff1a\u6570\u636e\u7531 <code>cudaMallocManaged()</code> \u51fd\u6570\u5206\u914d\uff0c\u5e76\u80fd\u7531 CPU \u8bbf\u95ee\u5904\u7406\u3002</li> <li><code>(2)</code> \u6bb5\uff1a\u6570\u636e\u53ef\u88ab\u8fc1\u79fb\u81f3\u53ef\u6267\u884c\u5e76\u884c\u5de5\u4f5c\u7684 GPU \uff0c\u5e76\u7531 GPU \u6838\u51fd\u6570\u8bbf\u95ee\uff0c\u540c\u65f6 CPU \u53ef\u7ee7\u7eed\u6267\u884c\u5de5\u4f5c\uff08\u5f02\u6b65\u6267\u884c\uff09\u3002</li> <li>\u901a\u8fc7 <code>cudaDeviceSynchronize()</code> \uff0c\u5c06 CPU \u4e0e GPU \u7684\u5de5\u4f5c\u540c\u6b65\u3002</li> <li><code>(3)</code> \u6bb5\uff1a\u7ecf CPU \u8bbf\u95ee\u7684\u6570\u636e\u8fc1\u79fb\u56de CPU\u3002</li> </ul>"},{"location":"HPC/HPC%20101/cudaC/#gpu_2","title":"GPU \u52a0\u901f\u5e94\u7528\u7a0b\u5e8f\u7f16\u5199","text":"<p>CUDA \u52a0\u901f\u7a0b\u5e8f\u6587\u4ef6\u6269\u5c55\u540d\u4e3a <code>.cu</code> \uff0c\u4e00\u4e2a\u7b80\u5355\u4f8b\u5b50\uff1a</p> <pre><code>void CPUFunction()\n{\nprintf(\"This function is defined to run on the CPU.\\n\");\n}\n\n__global__ void GPUFunction()\n{\nprintf(\"This function is defined to run on the GPU.\\n\");\n}\n\nint main()\n{\nCPUFunction();\n\nGPUFunction&lt;&lt;&lt;1, 1&gt;&gt;&gt;();\ncudaDeviceSynchronize();\n}\n</code></pre> <p>\u4e00\u4e9b\u8bed\u6cd5\uff1a</p> <p><code>__global__ void GPUFunction()</code></p> <ul> <li><code>__global__</code> \u5173\u952e\u5b57\u8868\u660e\u4ee5\u4e0b\u51fd\u6570\u5c06\u5728 GPU \u4e0a\u8fd0\u884c\u5e76\u53ef \u5168\u5c40 \u8c03\u7528\u3002</li> <li>\u901a\u5e38\uff0c\u5c06\u5728 CPU \u4e0a\u6267\u884c\u7684\u4ee3\u7801\u79f0\u4e3a \u4e3b\u673a (Host) \u4ee3\u7801\uff0c\u800c\u5c06\u5728 GPU \u4e0a\u8fd0\u884c\u7684\u4ee3\u7801\u79f0\u4e3a \u8bbe\u5907(Device) \u4ee3\u7801\u3002</li> <li>\u4f7f\u7528 <code>__global__</code> \u5173\u952e\u5b57\u5b9a\u4e49\u7684\u51fd\u6570\u9700\u8981\u8fd4\u56de <code>void</code> \u7c7b\u578b\u3002</li> </ul> <p><code>GPUFunction&lt;&lt;&lt;1, 1&gt;&gt;&gt;();</code></p> <ul> <li>\u901a\u5e38\uff0c\u5f53\u8c03\u7528\u8981\u5728 GPU \u4e0a\u8fd0\u884c\u7684\u51fd\u6570\u65f6\uff0c\u5c06\u6b64\u79cd\u51fd\u6570\u79f0\u4e3a \u5df2\u542f\u52a8 \u7684 \u6838\u51fd\u6570 \u3002</li> <li>\u542f\u52a8\u6838\u51fd\u6570\u65f6\uff0c\u5fc5\u987b\u63d0\u4f9b \u6267\u884c\u914d\u7f6e \uff0c\u5373\u5728\u5411\u6838\u51fd\u6570\u4f20\u9012\u53c2\u6570\u4e4b\u524d\u4f7f\u7528 <code>&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;</code> \u8bed\u6cd5\u5b8c\u6210\u914d\u7f6e\u3002</li> <li>\u5728\u5b8f\u89c2\u5c42\u9762\uff0c\u53ef\u901a\u8fc7\u6267\u884c\u914d\u7f6e\u4e3a\u6838\u51fd\u6570\u542f\u52a8\u6307\u5b9a \u7ebf\u7a0b\u5c42\u6b21\u7ed3\u6784 \uff0c\u4ece\u800c\u5b9a\u4e49\u7ebf\u7a0b\u7ec4\uff08\u79f0\u4e3a \u7ebf\u7a0b\u5757 \uff09\u7684\u6570\u91cf <code>number_of_blocks</code> \uff0c\u4ee5\u53ca\u8981\u5728\u6bcf\u4e2a\u7ebf\u7a0b\u5757\u4e2d\u6267\u884c\u7684 \u7ebf\u7a0b \u6570\u91cf <code>threads_per_block</code> \u3002\u5982\u6837\u4f8b\u4ee3\u7801\u4e2d\uff0c\u4f7f\u7528\u4e86\u5305\u542b <code>1</code> \u7ebf\u7a0b\u7684 <code>1</code> \u7ebf\u7a0b\u5757\u542f\u52a8\u6838\u51fd\u6570\u3002</li> </ul> <p><code>cudaDeviceSynchronize();</code></p> <ul> <li>\u6838\u51fd\u6570\u542f\u52a8\u65b9\u5f0f\u4e3a \u5f02\u6b65 \uff1aCPU \u4ee3\u7801\u5c06\u7ee7\u7eed\u6267\u884c\uff0c\u65e0\u9700\u7b49\u5f85\u6838\u51fd\u6570\u5b8c\u6210\u542f\u52a8\u3002</li> <li>\u8c03\u7528 CUDA \u8fd0\u884c\u65f6\u63d0\u4f9b\u7684\u51fd\u6570 <code>cudaDeviceSynchronize</code> \u4f7f\u5f97\u4e3b\u673a (CPU) \u4ee3\u7801\u6682\u4f5c\u7b49\u5f85\uff0c\u76f4\u81f3\u8bbe\u5907 (GPU) \u4ee3\u7801\u6267\u884c\u5b8c\u6210\uff0c\u624d\u80fd\u5728 CPU \u4e0a\u6062\u590d\u6267\u884c\u3002</li> </ul> <p>\u7f16\u8bd1\u8fd0\u884c CUDA \u52a0\u901f\u7a0b\u5e8f</p> <p>\u4f7f\u7528 <code>nvcc</code> \u547d\u4ee4\u7f16\u8bd1\u548c\u8fd0\u884c\u7a0b\u5e8f\uff1a</p> <pre><code>nvcc -arch=sm_70 -o hello-gpu hello-gpu.cu -run\n</code></pre> <ul> <li><code>nvcc</code> \u4e3a\u7f16\u8bd1\u5668\u547d\u4ee4\u3002</li> <li><code>-arch</code> \u9009\u9879\u6307\u5b9a\u67b6\u6784\u7c7b\u578b\u3002</li> <li><code>-o</code> \u6307\u5b9a\u7f16\u8bd1\u7a0b\u5e8f\u7684\u8f93\u51fa\u6587\u4ef6\u3002</li> <li><code>-run</code> \u6807\u5fd7\u6267\u884c\u6210\u529f\u7f16\u8bd1\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002</li> </ul>"},{"location":"HPC/HPC%20101/cudaC/#cuda","title":"CUDA \u7ebf\u6027\u5c42\u6b21\u7ed3\u6784","text":"<p>\u542f\u52a8\u6838\u51fd\u6570\u65f6\uff0c\u6838\u51fd\u6570\u4ee3\u7801\u7531\u6bcf\u4e2a\u5df2\u914d\u7f6e\u7684\u7ebf\u7a0b\u5757\u4e2d\u7684\u6bcf\u4e2a\u7ebf\u7a0b\u6267\u884c\u3002</p> <p>CUDA \u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217\u7ebf\u7a0b\u5c42\u6b21\u7ed3\u6784\u53d8\u91cf\uff1a</p> <ul> <li><code>gridDim.x</code> \u7f51\u683c\u4e2d\u5757\u6570\uff1b<code>blockDim.x</code> \u6bcf\u5757\u4e2d\u7ebf\u7a0b\u6570\u3002</li> <li><code>blockIdx.x</code> \u5f53\u524d\u7ebf\u7a0b\u6240\u5728\u7ebf\u7a0b\u5757\u7d22\u5f15\uff1b<code>threadIdx.x</code> \u5f53\u524d\u7ebf\u7a0b\u5728\u7ebf\u7a0b\u5757\u4e2d\u7684\u7d22\u5f15\u3002</li> </ul>"},{"location":"HPC/HPC%20101/cudaC/#_2","title":"\u52a0\u901f\u5faa\u73af","text":"<p>\u6b65\u9aa4\uff1a</p> <ul> <li>\u5fc5\u987b\u7f16\u5199\u5b8c\u6210 \u5faa\u73af\u7684\u5355\u6b21\u8fed\u4ee3 \u5de5\u4f5c\u7684\u6838\u51fd\u6570\u3002</li> <li>\u7531\u4e8e\u6838\u51fd\u6570\u4e0e\u5176\u4ed6\u6b63\u5728\u8fd0\u884c\u7684\u6838\u51fd\u6570\u65e0\u5173\uff0c\u56e0\u6b64\u6267\u884c\u914d\u7f6e\u5fc5\u987b\u4f7f\u6838\u51fd\u6570\u6267\u884c\u6b63\u786e\u7684\u6b21\u6570\uff0c\u4f8b\u5982\u5faa\u73af\u8fed\u4ee3\u7684\u6b21\u6570\u3002\uff08\u6ce8\u610f\uff1a\u5404\u4e2a\u7ebf\u7a0b\u6267\u884c\u987a\u5e8f\u4e0d\u5b9a\uff0c\u6545\u53ef\u52a0\u901f\u7684\u5faa\u73af\u9700\u8981\u5404\u6b21\u8fed\u4ee3\u65e0\u5173\u8054\u4e14\u987a\u5e8f\u65e0\u5f71\u54cd\uff09</li> </ul> <p>\u4f8b\u5b50\uff1a</p> <p>\u52a0\u901f\u524d\uff1a</p> <pre><code>for (int i = 0; i &lt; N; ++i)\n{\nprintf(\"%d\\n\", i);\n}\n</code></pre> <p>\u52a0\u901f\u540e\uff1a\u4f7f\u7528\u6838\u51fd\u6570\u5b9e\u73b0\u5355\u8bcd\u8fed\u4ee3</p> <pre><code>__global__ void loop() {\nint idx = threadIdx.x + blockIdx.x * blockDim.x;\nif (idx &lt; N)\nprintf(\"%d\\n\", idx);\n}\n</code></pre> <p>\u5e76\u8c03\u7528</p> <pre><code>loop&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;();\n</code></pre> <p>\u6b64\u5904\u5fc5\u987b\u6709 <code>number_of_blocks * threads_per_block &gt;= N</code> \u3002</p>"},{"location":"HPC/HPC%20101/cudaC/#gpu-cpu","title":"\u5206\u914d\u5c06\u8981\u5728 GPU \u548c CPU \u4e0a\u8bbf\u95ee\u7684\u5185\u5b58","text":"<p>\u56de\u5fc6\u4e00\u822c\u7684 CPU \u7a0b\u5e8f\u5206\u914d\u5e76\u91ca\u653e\u5185\u5b58\u7684\u65b9\u5f0f\uff1a</p> <pre><code>int *a;\na = (int *)malloc(size);\n...\nfree(a);\n</code></pre> <p>\u8981\u5206\u914d\u548c\u91ca\u653e\u5185\u5b58\uff0c\u5e76\u83b7\u53d6\u53ef\u5728\u4e3b\u673a\u548c\u8bbe\u5907\u4ee3\u7801\u4e2d\u5f15\u7528\u7684\u6307\u9488\uff0c\u5219\u9700\u8981\u4f7f\u7528 CUDA \u63d0\u4f9b\u7684\u51fd\u6570 <code>cudaMallocManaged()</code> \u548c <code>cudaFree()</code> \uff1a</p> <pre><code>int *a;\ncudaMallocManaged(&amp;a, size);\n...\ncudaFree(a);\n</code></pre>"},{"location":"HPC/HPC%20101/cudaC/#_3","title":"\u8de8\u7f51\u683c\u5faa\u73af","text":"<p>\u5f53\u6570\u636e\u96c6\u6bd4\u7f51\u683c\u5927\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u4f7f\u6bcf\u4e2a\u7ebf\u7a0b\u5904\u7406\u591a\u4e2a\u6570\u636e\uff1a</p> <pre><code>__global__ void kernel(int *a, int N)\n{\nint idx = threadIdx.x + blockIdx.x * blockDim.x;\nint stride = gridDim.x * blockDim.x;\n\nfor (int i = idx; i &lt; N; i += stride)\n{\n// do work on a[i];\n}\n}\n</code></pre>"},{"location":"HPC/HPC%20101/cudaC/#_4","title":"\u9519\u8bef\u5904\u7406","text":"<ul> <li>\u8bb8\u591a CUDA \u51fd\u6570\u4f1a\u8fd4\u56de\u7c7b\u578b\u4e3a <code>cudaError_t</code> \u7684\u503c\uff0c\u53ef\u7528\u4e8e\u68c0\u67e5\u8c03\u7528\u662f\u5426\u51fa\u9519\u3002</li> <li>\u4e3a\u68c0\u67e5\u542f\u52a8\u6838\u51fd\u6570\u65f6\u662f\u5426\u53d1\u751f\u9519\u8bef\uff0cCUDA \u63d0\u4f9b\u4e86 <code>cudaGetLastError</code> \u51fd\u6570\uff0c\u8fd4\u56de\u7c7b\u578b\u4e3a <code>cudaError_t</code> \u7684\u503c\u3002</li> <li>\u4e3a\u6355\u6349\u5f02\u6b65\u9519\u8bef\uff08\u4f8b\u5982\uff0c\u5728\u5f02\u6b65\u6838\u51fd\u6570\u6267\u884c\u671f\u95f4\uff09\uff0c\u9700\u8981\u68c0\u67e5\u540e\u7eed\u540c\u6b65 CUDA \u8fd0\u884c\u65f6 API \u8c03\u7528\u6240\u8fd4\u56de\u7684\u72b6\u6001\uff08\u4f8b\u5982 <code>cudaDeviceSynchronize</code> \uff09\uff0c\u5982\u679c\u4e4b\u524d\u542f\u52a8\u7684\u5176\u4e2d\u4e00\u4e2a\u6838\u51fd\u6570\u5931\u8d25\uff0c\u5219\u5c06\u8fd4\u56de\u9519\u8bef\u3002</li> </ul> <p>\u9519\u8bef\u5927\u81f4\u53ef\u5206\u4e3a\u540c\u6b65\u9519\u8bef <code>synError</code> \u548c\u5f02\u6b65\u9519\u8bef <code>asynError</code> \u3002</p> <p>\u5bf9\u4e8e <code>cudaError_t</code> \u503c\uff0c\u5c06\u5176\u4e0e CUDA \u63d0\u4f9b\u7684 <code>cudaSuccess</code> \u8fdb\u884c\u6bd4\u8f83\uff0c\u5373\u53ef\u5224\u65ad\u662f\u5426\u51fa\u9519\uff0c\u540c\u65f6\u53ef\u4ee5\u4f7f\u7528\u51fd\u6570 <code>cudaGetErrorString()</code> \u83b7\u5f97\u9519\u8bef\u4fe1\u606f\u3002</p> <p>\u4e00\u4e2a\u5305\u88c5 CUDA \u51fd\u6570\u8c03\u7528\u7684\u5b8f\uff1a</p> <pre><code>inline cudaError_t checkCuda(cudaError_t result)\n{\nif (result != cudaSuccess) {\nfprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\nassert(result == cudaSuccess);\n}\nreturn result;\n}\n\nint main()\n{\n\n/*\n * The macro can be wrapped around any function returning\n * a value of type `cudaError_t`.\n */\n\ncheckCuda( cudaDeviceSynchronize() )\n}\n</code></pre>"},{"location":"HPC/HPC%20101/cudaC/#-","title":"\u8fdb\u9636\u5185\u5bb9 - \u591a\u7ef4\u7f51\u683c\u548c\u5757","text":"<p>\u53ef\u4ee5\u5c06\u7f51\u683c\u548c\u7ebf\u7a0b\u5757\u5b9a\u4e49\u4e3a\u6700\u591a\u5177\u6709 3 \u4e2a\u7ef4\u5ea6\uff0c CUDA \u63d0\u4f9b <code>dim3</code> \u7c7b\u578b\u5b9a\u4e49\u591a\u7ef4\u7f51\u683c\u548c\u5757\uff1a</p> <pre><code>dim3 threads_per_block(block_dim_x, block_dim_y, block_dim_z);\ndim3 number_of_blocks(grid_dim_x, grid_dim_y, grid_dim_z);\nsomeKernel&lt;&lt;&lt;number_of_blocks, threads_per_block&gt;&gt;&gt;();\n</code></pre> <p>\u5728\u6838\u51fd\u6570\u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>threadIdx.y</code> \u53ca\u7c7b\u4f3c\u5f62\u5f0f\u83b7\u5f97\u76f8\u5173\u7d22\u5f15\u548c\u7ef4\u5ea6\u3002</p>"},{"location":"Labs/numpy/","title":"lab2 \u5411\u91cf\u5316\u8ba1\u7b97","text":""},{"location":"Labs/numpy/#_1","title":"\u601d\u8def","text":""},{"location":"Labs/numpy/#step-1","title":"Step 1","text":"<p>\u5148\u8003\u8651\u8ba1\u7b97\u5355\u5f20\u56fe\u7247\u7684\u60c5\u5f62\uff08 <code>N = 1</code> \uff09\u3002</p> <p>\u4e0b\u8bbe\u65b0\u7684 \\(H_2 \\times W_2\\) \u7684\u56fe\u4e2d \\((i,j)\\) \u5904\u50cf\u7d20\u6240\u60f3\u8981\u91c7\u6837\u7684 <code>a</code> \u56fe\u4e2d\u5bf9\u5e94\u70b9\u5750\u6807\u4e3a \\((x_{(i,j)}, y_{(i,j)})\\) \uff0c\u5373 <code>b[n, i, j]</code> \u5904\u6240\u5b58\u5750\u6807\u3002</p> <p>\u9605\u8bfb <code>baseline.py</code> \uff0c\u53ef\u77e5\u57fa\u51c6\u4ee3\u7801\u91c7\u7528\u4e86\u5982\u4e0b\u7b49\u5f0f\u8ba1\u7b97\u6bcf\u4e2a\u91c7\u6837\u70b9 <code>(x, y)</code> \u7684\u91c7\u6837\u7ed3\u679c\uff1a</p> \\[ \\begin{aligned}     f(x,y) \\approx     {\\frac {1}{(x_{2}-x_{1})(y_{2}-y_{1})}}      {\\big (}     &amp;f(Q_{11})(x_{2}-x)(y_{2}-y) \\\\     +&amp;f(Q_{21})(x-x_{1})(y_{2}-y)\\\\     +&amp;f(Q_{12})(x_{2}-x)(y-y_{1})\\\\     +&amp;f(Q_{22})(x-x_{1})(y-y_{1})     {\\big )} \\end{aligned} \\] <p>\u5728\u672c\u6b21\u5b9e\u9a8c\u7ed9\u51fa\u7684\u60c5\u666f\u4e2d\uff0c \\(x_2 - x_1 = y_2 - y_1 = 1, x_1 = \\lfloor x \\rfloor, y_1 = \\lfloor y \\rfloor\\)  \uff0c\u8bb0  \\(x' = x - \\lfloor x \\rfloor\uff0c y' = y - \\lfloor y \\rfloor\\)  \uff0c\u4e0a\u8ff0\u7b49\u5f0f\u53ef\u7b80\u5316\u4e3a\uff1a</p> \\[ \\displaystyle \\begin{aligned}     f(x,y) \\approx      \\ &amp; f(Q_{11})(1 - x')(1 - y')+f(Q_{21})(x')(1 - y') \\\\     + \\ &amp; f(Q_{12})(1 - x')(y')+f(Q_{22})(x')(y') \\\\ \\end{aligned} \\] <p>\u7531\u4e8e\u6bcf\u4e2a\u91c7\u6837\u70b9\u7684\u8ba1\u7b97\u90fd\u662f\u72ec\u7acb\u7684\uff0c\u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49\u5982\u4e0b\u51e0\u4e2a\u77e9\u9635\uff0c</p> \\[ \\mathbf{F} = \\big( f(x_{(i,j)}, y_{(i,j)})\\big)_{H2 \\times W2}  \\] \\[ \\begin{aligned}     \\mathbf{F_{00}} = \\big( f(Q_{11})\\big)_{H2 \\times W2} \\quad     \\mathbf{F_{01}} = \\big( f(Q_{12})\\big)_{H2 \\times W2} \\\\     \\mathbf{F_{10}} = \\big( f(Q_{21})\\big)_{H2 \\times W2} \\quad     \\mathbf{F_{11}} = \\big( f(Q_{22})\\big)_{H2 \\times W2} \\\\ \\end{aligned} \\] \\[ \\begin{aligned}     \\mathbf{X'} = \\big( x'_{(i, j)} \\big)_{H2 \\times W2} \\\\     \\mathbf{Y'} = \\big( y'_{(i, j)} \\big)_{H2 \\times W2} \\end{aligned} \\] <p>\u4f7f\u7528\u9010\u5143\u7d20\u4e58\u6cd5\uff08 <code>*</code> \uff09\u548c\u77e9\u9635\u52a0\u6cd5\u5373\u53ef\u5b9e\u73b0\u5411\u91cf\u5316\u8ba1\u7b97\u77e9\u9635 \\(\\mathbf{F}\\) \u4e2d\u7684\u91c7\u6837\u4fe1\u606f\uff1a</p> \\[ \\begin{aligned}     \\mathbf{F}     &amp; = \\mathbf{F_{00}} * (1 - \\mathbf{X'}) * (1 - \\mathbf{Y'}) + \\mathbf{F_{10}} * \\mathbf{X'} * (1 - \\mathbf{Y'}) \\\\     &amp; + \\mathbf{F_{01}} * (1 - \\mathbf{X'}) * \\mathbf{Y'} + \\mathbf{F_{11}} * \\mathbf{X'} * \\mathbf{Y'}  \\end{aligned} \\] <p>\u636e\u6b64\u5373\u53ef\u4f7f\u7528\u5411\u91cf\u5316\u53bb\u6389\u5185\u5c42\u4e24\u5c42 <code>for</code> \u5faa\u73af\uff0c\u4ee3\u7801\u5b9e\u73b0\u5982\u4e0b\uff1a</p> vectorize1.py<pre><code>def bilinear_interp_vectorized(a: np.ndarray, b: np.ndarray) \\\n    -&gt; np.ndarray:\n\"\"\"\n    This is the vectorized implementation of bilinear interpolation.\n    - a is a ND array with shape [N, H1, W1, C], dtype = int64\n    - b is a ND array with shape [N, H2, W2, 2], dtype = float64\n    - return a ND array with shape [N, H2, W2, C], dtype = int64\n    \"\"\"\n    # get axis size from ndarray shape\n    N, H1, W1, C = a.shape\n    N1, H2, W2, _ = b.shape\n    assert N == N1\n\n    # Do iteration\n    b = b.transpose(0, 3, 1, 2)\n    res = np.empty((N, H2, W2, C), dtype=int64)\n\n    for n in range(N):\n        X, Y = b[n]\n        X_idx , Y_idx = np.floor(X).astype(int64), \\\n                        np.floor(Y).astype(int64)\n        _X, _Y = X - X_idx, Y - Y_idx\n        A00 = a[n, X_idx, Y_idx].transpose(2, 0, 1)\n        A01 = a[n, X_idx, Y_idx + 1].transpose(2, 0, 1)\n        A10 = a[n, X_idx + 1, Y_idx].transpose(2, 0, 1)\n        A11 = a[n, X_idx + 1, Y_idx + 1].transpose(2, 0, 1)\n        res00 = A00 * (1 - _X) * (1 - _Y)\n        res10 = A10 * _X * (1 - _Y)\n        res01 = A01 * (1 - _X) * _Y\n        res11 = A11 * _X * _Y\n        res[n] = res00.transpose(1, 2, 0) + \\\n                 res01.transpose(1, 2, 0) + \\\n                 res10.transpose(1, 2, 0) + \\\n                 res11.transpose(1, 2, 0)\n    return res\n</code></pre> <p>Note</p> <p>\u4ee3\u7801\u4e2d\u4f7f\u7528\u4e86 <code>np.transpose</code> \u51fd\u6570\u8fdb\u884c\u77e9\u9635\u8f6c\u7f6e\uff0c\u4f7f\u5f97 <code>X, Y</code> \u80fd\u5bb9\u6613\u5730\u4ece <code>b</code> \u4e2d\u53d6\u51fa\uff0c\u5e76\u4e14\u4f7f\u5f97\u5728\u8fdb\u884c\u77e9\u9635\u9010\u5143\u7d20\u4e58\u6cd5\u65f6\u80fd\u6ee1\u8db3\u5e7f\u64ad\u89c4\u5219\uff0c\u540c\u65f6\u5bf9 <code>C</code> \u4e2a\u901a\u9053\u7684\u4fe1\u606f\u8fdb\u884c\u5904\u7406\u3002</p>"},{"location":"Labs/numpy/#step-2","title":"Step 2","text":"<p>\u73b0\u5728\u8003\u8651\u540c\u65f6\u5bf9 <code>N</code> \u5f20\u56fe\u7247\u8fdb\u884c\u5904\u7406\u3002</p> <p>\u5bf9 <code>vectorize1.py</code> \u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u4fee\u6539\u3002</p> <p>\u6211\u4eec\u8981\u5c06 <code>N</code> \u5f20\u56fe\u7247\u7684\u4fe1\u606f\u8fdb\u884c\u6574\u5408\u5904\u7406\uff0c\u4e00\u4e2a\u53ef\u884c\u7684\u65b9\u6cd5\u4e3a\u5728 <code>X, Y</code> \u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e00\u4e2a\u4ee3\u8868\u56fe\u7247\u7d22\u5f15\u7684\u8f74\uff0c\u4f7f\u5176\u5f62\u72b6\u4ece <code>(H2, W2)</code> \u53d8\u4e3a <code>(N, H2, W2)</code> \u3002\u5177\u4f53\u5b9e\u73b0\u5982\u4e0b\uff08 <code>X_idx, Y_idx, _X, _Y</code> \u7684\u83b7\u53d6\u4e00\u5e76\u7ed9\u51fa\uff09\uff1a</p> <pre><code>X, Y = b.transpose(3, 0, 1, 2)\nX_idx , Y_idx = np.floor(X).astype(int64), np.floor(Y).astype(int64)\n_X, _Y = X - X_idx, Y - Y_idx\n</code></pre> <p>\u540c\u7406\uff0c<code>A00, A01, A10, A11</code> \u5747\u9700\u8981\u589e\u52a0\u4e00\u4e2a\u8f74\uff0c\u4e8e\u662f\u65b0\u589e\u4e00\u4e2a\u7d22\u5f15\u6570\u7ec4 <code>N_idx</code> \u5bf9 <code>a</code> \u6570\u7ec4\u7684\u7b2c\u4e00\u4e2a\u8f74 \uff08 <code>axis = 0</code> \uff09 \u8fdb\u884c\u7d22\u5f15\u3002\u5177\u4f53\u5b9e\u73b0\u5982\u4e0b\uff1a</p> <pre><code>N_idx = np.arange(N).reshape(N, 1, 1)\n\nA00 = a[N_idx, X_idx, Y_idx].transpose(3, 0, 1, 2)\nA01 = a[N_idx, X_idx, Y_idx + 1].transpose(3, 0, 1, 2)\nA10 = a[N_idx, X_idx + 1, Y_idx].transpose(3, 0, 1, 2)\nA11 = a[N_idx, X_idx + 1, Y_idx + 1].transpose(3, 0, 1, 2)\n</code></pre> <p>Note</p> <p><code>N_idx</code> \u7684\u5f62\u72b6\u4e0e <code>X_idx, Y_idx</code> \u5e76\u4e0d\u5339\u914d\uff0c\u4e4b\u6240\u4ee5\u80fd\u591f\u50cf\u8fd9\u6837\u5bf9 <code>a</code> \u6570\u7ec4\u8fdb\u884c fancy indexing \uff0c\u662f\u56e0\u4e3a\u8fdb\u884c\u7d22\u5f15\u7684\u65f6\u5019\u5bf9 <code>N_idx</code> \u7684\u540e\u4e24\u4e2a\u8f74\uff08 <code>axis=1, axis=2</code> \uff09\u8fdb\u884c\u4e86 broadcast \u3002</p> <p>\u5b8c\u6574\u4ee3\u7801\u5982\u4e0b\uff1a</p> vectorize.py<pre><code>def bilinear_interp_vectorized(a: np.ndarray, b: np.ndarray) \\\n    -&gt; np.ndarray:\n\"\"\"\n    This is the vectorized implementation of bilinear interpolation.\n    - a is a ND array with shape [N, H1, W1, C], dtype = int64\n    - b is a ND array with shape [N, H2, W2, 2], dtype = float64\n    - return a ND array with shape [N, H2, W2, C], dtype = int64\n    \"\"\"\n    # get axis size from ndarray shape\n    N, _, _, C = a.shape\n    N1, H2, W2, _ = b.shape\n    assert N == N1\n\n    # Do iteration\n    res = np.empty((N, H2, W2, C), dtype=int64)\n\n    # Get the matrices of coordinates\n    X, Y = b.transpose(3, 0, 1, 2)\n    X_idx , Y_idx = np.floor(X).astype(int64), \\\n                    np.floor(Y).astype(int64)\n    _X, _Y = X - X_idx, Y - Y_idx\n\n    # Generate the indices array\n    N_idx = np.arange(N).reshape(N, 1, 1)\n\n    A00 = a[N_idx, X_idx, Y_idx].transpose(3, 0, 1, 2)\n    A01 = a[N_idx, X_idx, Y_idx + 1].transpose(3, 0, 1, 2)\n    A10 = a[N_idx, X_idx + 1, Y_idx].transpose(3, 0, 1, 2)\n    A11 = a[N_idx, X_idx + 1, Y_idx + 1].transpose(3, 0, 1, 2)\n    res = A00 * (1 - _X) * (1 - _Y) + A01 * (1 - _X) * _Y + \\\n          A10 * _X * (1 - _Y) + A11 * _X * _Y\n    return res.transpose(1, 2, 3, 0).astype(int64)\n</code></pre>"},{"location":"Labs/numpy/#_2","title":"\u6b63\u786e\u6027\u4e0e\u52a0\u901f\u6bd4","text":"<p>\u8fd0\u884c <code>main.py</code> \uff0c\u67e5\u770b\u8f93\u51fa\uff1a</p> <pre><code>Generating Data...\nExecuting Baseline Implementation...\nFinished in 99.58129525184631s\nExecuting Vectorized Implementation...\nFinished in 3.138871192932129s\n[PASSED] Results are identical.\nSpeed Up 31.725193272051396x\n</code></pre> <p>\u53ef\u89c1\u8fd0\u884c\u7ed3\u679c\u6b63\u786e\uff0c\u52a0\u901f\u6bd4\u4e3a 31.725193272051396 \u3002</p>"},{"location":"Labs/numpy/#reference","title":"Reference","text":"<ul> <li>\u521d\u63a2Numpy\u4e2d\u7684\u82b1\u5f0f\u7d22\u5f15\uff1ahttps://zhuanlan.zhihu.com/p/123858781</li> <li>Numpy\u4e2dtranspose()\u51fd\u6570\u7684\u53ef\u89c6\u5316\u7406\u89e3\uff1ahttps://zhuanlan.zhihu.com/p/61203757</li> <li>NumPy \u4e2d\u6587\u6587\u6863\uff0c\u5e7f\u64ad\uff08Broadcasting\uff09\uff1ahttps://numpy.org.cn/user/basics/broadcasting.html</li> </ul>"}]}